{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837bf38c",
   "metadata": {},
   "source": [
    "# Nhận dạng & phân loại khuẩn lạc vi sinh vật bằng Ultralytics YOLO (Detect 24 classes)\n",
    "\n",
    "Notebook này chạy end-to-end trên **Kaggle**:\n",
    "1) **Tải dữ liệu** từ Figshare (qua Figshare REST API) *(cần bật Internet trong Kaggle Notebook Settings)*  \n",
    "2) **Chuẩn hoá** dataset sang format Ultralytics YOLO + **train/val/test split**  \n",
    "3) **Train YOLO** (object detection + classification theo 24 loài)  \n",
    "4) **Evaluate** + **Inference demo**  \n",
    "5) **Export** (best.pt + ONNX)\n",
    "\n",
    "Dataset nguồn: *Annotated dataset for deep-learning-based bacterial colony detection* (Makrai et al., Scientific Data 2023) — 369 ảnh, 24 loài, 56,865 khuẩn lạc (bbox + label).\n",
    "\n",
    "> Gợi ý: Nếu download quá chậm, bạn có thể **tải dữ liệu về máy**, upload thành **Kaggle Dataset**, rồi đặt `DOWNLOAD_FROM_FIGSHARE = False` và trỏ `RAW_DIR` sang `/kaggle/input/...`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d94de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 0) Setup ===\n",
    "# Bật GPU: Settings -> Accelerator -> GPU\n",
    "# Bật Internet: Settings -> Internet -> On (để download Figshare)\n",
    "\n",
    "!pip install -q ultralytics opencv-python matplotlib pandas pyyaml requests scikit-learn tqdm\n",
    "\n",
    "import os, re, json, zipfile, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4d9eb",
   "metadata": {},
   "source": [
    "## 1) Tải dữ liệu từ Figshare (hoặc dùng Kaggle Input)\n",
    "\n",
    "Mặc định notebook sẽ:\n",
    "- Tải `annot_YOLO.zip`\n",
    "- Tải toàn bộ ảnh `*.jpg`\n",
    "\n",
    "Figshare web UI có thể chặn crawler, nên mình dùng **Figshare REST API** để lấy `download_url` trực tiếp cho từng file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Download dataset ===\n",
    "\n",
    "DOWNLOAD_FROM_FIGSHARE = True\n",
    "\n",
    "FIGSHARE_ARTICLE_ID = 22022540\n",
    "RAW_DIR = Path(\"/kaggle/working/raw_figshare\")  # nơi lưu file download\n",
    "\n",
    "# Nếu bạn đã add dataset vào Kaggle Input, đặt:\n",
    "# DOWNLOAD_FROM_FIGSHARE = False\n",
    "# RAW_DIR = Path(\"/kaggle/input/<your-dataset-folder>\")\n",
    "\n",
    "FIGSHARE_API = \"https://api.figshare.com/v2\"\n",
    "\n",
    "def figshare_get_json(url: str, timeout: int = 60):\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def figshare_list_files(article_id: int):\n",
    "    return figshare_get_json(f\"{FIGSHARE_API}/articles/{article_id}/files\")\n",
    "\n",
    "def figshare_file_details(article_id: int, file_id: int):\n",
    "    return figshare_get_json(f\"{FIGSHARE_API}/articles/{article_id}/files/{file_id}\")\n",
    "\n",
    "def download_url_to_path(download_url: str, out_path: Path, chunk_size: int = 1024*1024):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with requests.get(download_url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", \"0\") or \"0\")\n",
    "        pbar = tqdm(total=total, unit=\"B\", unit_scale=True, desc=out_path.name)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "        pbar.close()\n",
    "\n",
    "def download_figshare_article(article_id: int, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    files = figshare_list_files(article_id)\n",
    "\n",
    "    # Chỉ tải YOLO annotation zip + jpg images\n",
    "    wanted = []\n",
    "    for f in files:\n",
    "        name = f.get(\"name\", \"\")\n",
    "        if name == \"annot_YOLO.zip\" or name.lower().endswith(\".jpg\") or name.lower().endswith(\".jpeg\"):\n",
    "            wanted.append(f)\n",
    "\n",
    "    print(f\"Found {len(files)} files on Figshare; will download {len(wanted)} file(s).\")\n",
    "\n",
    "    for f in wanted:\n",
    "        fid = int(f[\"id\"])\n",
    "        name = str(f[\"name\"])\n",
    "        out_path = out_dir / name\n",
    "        if out_path.exists():\n",
    "            continue\n",
    "\n",
    "        details = figshare_file_details(article_id, fid)\n",
    "        download_url = details.get(\"download_url\")\n",
    "        if not download_url:\n",
    "            raise RuntimeError(f\"No download_url for file {name} (id={fid})\")\n",
    "\n",
    "        print(f\"Downloading: {name}\")\n",
    "        download_url_to_path(download_url, out_path)\n",
    "\n",
    "if DOWNLOAD_FROM_FIGSHARE:\n",
    "    download_figshare_article(FIGSHARE_ARTICLE_ID, RAW_DIR)\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"Has annot_YOLO.zip:\", (RAW_DIR/\"annot_YOLO.zip\").exists())\n",
    "n_imgs = len(list(RAW_DIR.glob(\"*.jpg\"))) + len(list(RAW_DIR.glob(\"*.JPG\")))\n",
    "print(\"Number of images:\", n_imgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bcb98",
   "metadata": {},
   "source": [
    "## 2) Chuẩn hoá dataset về format Ultralytics YOLO + chia train/val/test\n",
    "\n",
    "Ultralytics YOLO cần cấu trúc kiểu:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "  images/train, images/val, images/test\n",
    "  labels/train, labels/val, labels/test\n",
    "  data.yaml\n",
    "```\n",
    "\n",
    "(Trong `data.yaml` có `path`, `train`, `val`, `names`...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2) Prepare dataset ===\n",
    "\n",
    "OUT_DIR = Path(\"/kaggle/working/yolo_dataset\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_CLASS_NAMES = [\n",
    "    \"Actinobacillus equuli\",\n",
    "    \"Actinobacillus pleuropneumoniae\",\n",
    "    \"Aeromonas hydrophila\",\n",
    "    \"Bacillus cereus\",\n",
    "    \"Bibersteinia trehalosi\",\n",
    "    \"Bordetella bronchiseptica\",\n",
    "    \"Brucella ovis\",\n",
    "    \"Clostridium perfringens\",\n",
    "    \"Corynebacterium pseudotuberculosis\",\n",
    "    \"Erysipelothrix rhusiopathiae\",\n",
    "    \"Escherichia coli\",\n",
    "    \"Glaesserella parasuis\",\n",
    "    \"Klebsiella pneumoniae\",\n",
    "    \"Listeria monocytogenes\",\n",
    "    \"Paenibacillus larvae\",\n",
    "    \"Pasteurella multocida\",\n",
    "    \"Proteus mirabilis\",\n",
    "    \"Pseudomonas aeruginosa\",\n",
    "    \"Rhodococcus equi\",\n",
    "    \"Salmonella enterica\",\n",
    "    \"Staphylococcus aureus\",\n",
    "    \"Staphylococcus hyicus\",\n",
    "    \"Streptococcus agalactiae\",\n",
    "    \"Trueperella pyogenes\",\n",
    "]\n",
    "\n",
    "SPECIES_RE = re.compile(r\"^(sp\\d{2})_img\\d+\", re.IGNORECASE)\n",
    "\n",
    "def infer_species_id(filename: str):\n",
    "    m = SPECIES_RE.match(Path(filename).stem)\n",
    "    return m.group(1).lower() if m else \"unknown\"\n",
    "\n",
    "def unzip(zip_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(out_dir)\n",
    "\n",
    "def find_label_files(labels_root: Path):\n",
    "    txts = []\n",
    "    for p in labels_root.rglob(\"*.txt\"):\n",
    "        if p.name.lower() in {\"classes.txt\", \"obj.names\"}:\n",
    "            continue\n",
    "        txts.append(p)\n",
    "    return sorted(txts)\n",
    "\n",
    "def find_classes_file(labels_root: Path):\n",
    "    for name in [\"classes.txt\", \"obj.names\"]:\n",
    "        p = labels_root / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for p in labels_root.rglob(\"classes.txt\"):\n",
    "        return p\n",
    "    for p in labels_root.rglob(\"obj.names\"):\n",
    "        return p\n",
    "    return None\n",
    "\n",
    "def load_class_names(labels_root: Path):\n",
    "    cf = find_classes_file(labels_root)\n",
    "    if cf:\n",
    "        lines = [ln.strip() for ln in cf.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            return lines\n",
    "    return DEFAULT_CLASS_NAMES\n",
    "\n",
    "def parse_label_line(line: str):\n",
    "    parts = line.strip().split()\n",
    "    cls = int(float(parts[0]))\n",
    "    x, y, w, h = map(float, parts[1:5])\n",
    "    return cls, x, y, w, h\n",
    "\n",
    "def scan_class_range(label_files):\n",
    "    mn, mx = 10**9, -10**9\n",
    "    for lf in label_files:\n",
    "        for line in lf.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            cls, *_ = parse_label_line(line)\n",
    "            mn = min(mn, cls)\n",
    "            mx = max(mx, cls)\n",
    "    return mn, mx\n",
    "\n",
    "def rewrite_labels(label_files, image_stems, out_labels_dir, class_shift: int):\n",
    "    out_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    mapping = {}\n",
    "    for lf in tqdm(label_files, desc=\"Rewrite labels\"):\n",
    "        stem = lf.stem\n",
    "        if stem not in image_stems:\n",
    "            continue\n",
    "        out_path = out_labels_dir / f\"{stem}.txt\"\n",
    "        out_lines = []\n",
    "        for line in lf.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            cls, x, y, w, h = parse_label_line(line)\n",
    "            cls2 = cls + class_shift\n",
    "            out_lines.append(f\"{cls2} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\")\n",
    "        out_path.write_text(\"\\n\".join(out_lines) + (\"\\n\" if out_lines else \"\"), encoding=\"utf-8\")\n",
    "        mapping[stem] = out_path\n",
    "    return mapping\n",
    "\n",
    "def copy_split(image_paths, labels_by_stem, images_out, labels_out):\n",
    "    images_out.mkdir(parents=True, exist_ok=True)\n",
    "    labels_out.mkdir(parents=True, exist_ok=True)\n",
    "    for img in tqdm(image_paths, desc=f\"Copy {images_out.name}\"):\n",
    "        stem = img.stem\n",
    "        shutil.copy2(img, images_out / img.name)\n",
    "        lf = labels_by_stem.get(stem)\n",
    "        if lf and lf.exists():\n",
    "            shutil.copy2(lf, labels_out / f\"{stem}.txt\")\n",
    "        else:\n",
    "            (labels_out / f\"{stem}.txt\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def write_data_yaml(out_dir: Path, class_names):\n",
    "    data = {\n",
    "        \"path\": str(out_dir.resolve()),\n",
    "        \"train\": \"images/train\",\n",
    "        \"val\": \"images/val\",\n",
    "        \"test\": \"images/test\",\n",
    "        \"names\": {i: n for i, n in enumerate(class_names)},\n",
    "    }\n",
    "    yaml_path = out_dir / \"data.yaml\"\n",
    "    yaml_path.write_text(yaml.safe_dump(data, sort_keys=False, allow_unicode=True), encoding=\"utf-8\")\n",
    "    return yaml_path\n",
    "\n",
    "# Images\n",
    "image_paths = sorted(list(RAW_DIR.glob(\"*.jpg\")) + list(RAW_DIR.glob(\"*.JPG\")))\n",
    "if not image_paths:\n",
    "    image_paths = sorted(list(RAW_DIR.rglob(\"*.jpg\")) + list(RAW_DIR.rglob(\"*.JPG\")))\n",
    "print(\"Images:\", len(image_paths))\n",
    "\n",
    "# Labels\n",
    "labels_root = OUT_DIR / \"_labels_raw\"\n",
    "if not labels_root.exists():\n",
    "    unzip(RAW_DIR / \"annot_YOLO.zip\", labels_root)\n",
    "\n",
    "label_files = find_label_files(labels_root)\n",
    "print(\"Label files:\", len(label_files))\n",
    "\n",
    "class_names = load_class_names(labels_root)\n",
    "nc = len(class_names)\n",
    "\n",
    "mn, mx = scan_class_range(label_files)\n",
    "class_shift = 0\n",
    "if mn == 1 and mx == nc:\n",
    "    class_shift = -1\n",
    "print(f\"class range: min={mn}, max={mx}, nc={nc}, shift={class_shift}\")\n",
    "\n",
    "labels_clean_dir = OUT_DIR / \"_labels_clean\"\n",
    "image_stems = {p.stem for p in image_paths}\n",
    "labels_by_stem = rewrite_labels(label_files, image_stems, labels_clean_dir, class_shift)\n",
    "\n",
    "# Split (stratified by species id inferred from filename)\n",
    "y = [infer_species_id(p.name) for p in image_paths]\n",
    "\n",
    "seed = 42\n",
    "test_ratio = 0.15\n",
    "val_ratio = 0.15\n",
    "val_ratio_rel = val_ratio / (1.0 - test_ratio)\n",
    "\n",
    "X_trainval, X_test, y_trainval, _ = train_test_split(image_paths, y, test_size=test_ratio, random_state=seed, stratify=y)\n",
    "X_train, X_val, _, _ = train_test_split(X_trainval, y_trainval, test_size=val_ratio_rel, random_state=seed, stratify=y_trainval)\n",
    "\n",
    "for split_name, X in [(\"train\", X_train), (\"val\", X_val), (\"test\", X_test)]:\n",
    "    copy_split(X, labels_by_stem, OUT_DIR/\"images\"/split_name, OUT_DIR/\"labels\"/split_name)\n",
    "\n",
    "DATA_YAML = write_data_yaml(OUT_DIR, class_names)\n",
    "\n",
    "print(\"✅ Prepared dataset at:\", OUT_DIR)\n",
    "print(\"✅ data.yaml:\", DATA_YAML)\n",
    "print(\"Split sizes:\", len(X_train), len(X_val), len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5c0a4",
   "metadata": {},
   "source": [
    "## 3) Train YOLO (Detect + 24-class)\n",
    "\n",
    "Vì khuẩn lạc rất nhỏ và ảnh thường độ phân giải cao, bạn nên bắt đầu với:\n",
    "- `imgsz=1024` (nếu GPU yếu thì giảm 640)\n",
    "- `batch=4` (giảm nếu OOM)\n",
    "- model nhỏ: `yolov8n.pt` hoặc `yolov8s.pt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3) Train ===\n",
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL = \"yolov8n.pt\"  # thử 'yolov8s.pt' nếu GPU ổn hơn\n",
    "EPOCHS = 50\n",
    "IMGSZ = 1024\n",
    "BATCH = 4\n",
    "\n",
    "model = YOLO(MODEL)\n",
    "\n",
    "results = model.train(\n",
    "    data=str(DATA_YAML),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    project=\"/kaggle/working/runs/detect\",\n",
    "    name=\"bacterial_colony_24cls\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151787a7",
   "metadata": {},
   "source": [
    "## 4) Evaluate + Inference demo\n",
    "\n",
    "- `model.val(...)` để đánh giá (mAP, precision/recall, v.v.)\n",
    "- `model.predict(...)` để demo ảnh test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4) Evaluate ===\n",
    "best_pt = \"/kaggle/working/runs/detect/bacterial_colony_24cls/weights/best.pt\"\n",
    "model = YOLO(best_pt)\n",
    "\n",
    "metrics_val = model.val(data=str(DATA_YAML), split=\"val\", imgsz=IMGSZ)\n",
    "print(metrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5) Predict on a few test images ===\n",
    "test_dir = OUT_DIR / \"images\" / \"test\"\n",
    "pred = model.predict(source=str(test_dir), imgsz=IMGSZ, conf=0.25, save=True,\n",
    "                     project=\"/kaggle/working/runs/predict\", name=\"demo\")\n",
    "print(\"Saved predictions to:\", \"/kaggle/working/runs/predict/demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe467",
   "metadata": {},
   "source": [
    "## 5) Export (best.pt + ONNX)\n",
    "\n",
    "Ultralytics hỗ trợ export sang nhiều định dạng (ONNX, TensorRT, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6) Export ===\n",
    "# Copy best.pt ra working root (để dễ tải về)\n",
    "import shutil, os\n",
    "shutil.copy2(best_pt, \"/kaggle/working/best.pt\")\n",
    "print(\"Saved:\", \"/kaggle/working/best.pt\")\n",
    "\n",
    "# Export ONNX\n",
    "model.export(format=\"onnx\", imgsz=IMGSZ)\n",
    "print(\"Done export\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
